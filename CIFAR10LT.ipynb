{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5740a1b7-ee51-4cd9-a943-36be993f6308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import matplotlib.pyplot as plt    # to visualize data and draw plots\n",
    "from tqdm import tqdm              # to track progress of loops\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8868d473-867b-4be4-a383-fc4f59b3555e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the CIFAR-10 test dataset for testing.\n",
    "def prepare_test_cifar10(): \n",
    "    test_ds = tfds.load('cifar10', split='test')\n",
    "    \n",
    "    prep_test_ds = []\n",
    "    for data in test_ds: \n",
    "        prep_test_ds.append(data)\n",
    "        \n",
    "    return prep_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b061b542-c6aa-4e29-a888-50a81ca835d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the balanced CIFAR-10 train dataset for training. \n",
    "# NOTE: Probably did it in the most inefficient way possible. \n",
    "def prepare_train_cifar10(): \n",
    "    # Load CIFAR-10 dataset\n",
    "    train_ds, info = tfds.load('cifar10', split='train', with_info=True)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = info.features['label'].names\n",
    "\n",
    "    # Create lists to hold images and labels in order from class 0 to class 9\n",
    "    ordered_images = [[] for _ in range(10)]\n",
    "    ordered_labels = [[] for _ in range(10)]\n",
    "\n",
    "    # Iterate through the dataset and sort images and labels\n",
    "    for example in train_ds:\n",
    "        image, label = example['image'], example['label']\n",
    "        ordered_images[label].append(image)\n",
    "        ordered_labels[label].append(example['label'])\n",
    "\n",
    "    # Concatenate lists\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(10):\n",
    "        images.extend(ordered_images[i])\n",
    "        labels.extend(ordered_labels[i])\n",
    "\n",
    "    # Convert lists to TensorFlow tensors\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "    \n",
    "    prep_train_ds = []\n",
    "    for i in range(50000): \n",
    "        prep_train_ds.append({\n",
    "            'id': i,\n",
    "            'image': images[i],\n",
    "            'label': labels[i]\n",
    "        })\n",
    "        \n",
    "    return prep_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346a3f8e-e2f9-4ac8-a571-33e4f530223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the balanced CIFAR-10 train dataset as a parameter and imbalances it \n",
    "def prepare_imb_train_cifar10(bal_dataset):\n",
    "    prep_imb_train_ds = []\n",
    "    balanced_dataset = bal_dataset\n",
    "    \n",
    "    #Remove a certain amount of data for each class in CIFAR-10. The amount of data removed is incremated by a rate of 0.1.\n",
    "    for i in range(10): \n",
    "        prep_imb_train_ds.append(balanced_dataset[(5000*i):(5000 *(i+1))])\n",
    "\n",
    "        number_remove = int(5000 * (0.1*i))\n",
    "        del prep_imb_train_ds[i][:number_remove]\n",
    "    \n",
    "    prep_imb_train_ds = sum(prep_imb_train_ds, [])\n",
    "    \n",
    "    return prep_imb_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebd68458-4598-41aa-b50f-c096b8c94df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(lst):\n",
    "    random.shuffle(lst)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a04f45e8-eed0-4bcb-9ca2-eff55b114d08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-02 23:30:01.973507: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-02 23:30:03.700293: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 30979 MB memory:  -> device: 0, name: Tesla V100-PCIE-32GB, pci bus id: 0000:5e:00.0, compute capability: 7.0\n",
      "2024-05-02 23:30:04.050718: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "test_cifar10_ds = prepare_test_cifar10()\n",
    "train_cifar10_ds = prepare_train_cifar10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "176ccda3-17a2-42cd-b86e-93f6fd2bc6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "imb_train_cifar10_ds = prepare_imb_train_cifar10(train_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "358649c7-6cbf-4d43-9120-3bc8ef30a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### MODEL ARCH #########\n",
    "hidden_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "hidden_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "pool_1 = tf.keras.layers.MaxPool2D(padding='same')\n",
    "hidden_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "hidden_4 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "pool_2 = tf.keras.layers.MaxPool2D(padding='same')\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "output = tf.keras.layers.Dense(10)\n",
    "conv_classifier = tf.keras.Sequential([hidden_1, hidden_2, pool_1, hidden_3, hidden_4, pool_2, flatten, output])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a256a89-5017-4e35-be7a-fd622026bbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_test_train(): \n",
    "    shuffle_list(test_cifar10_ds)\n",
    "    shuffle_list(train_cifar10_ds)\n",
    "    shuffle_list(imb_train_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "654fc632-1dae-49ce-be76-06990baf1b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:01<00:00, 16.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy During Training: 0.23181818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_values = []\n",
    "accuracy_values = []\n",
    "# Early Stopping Parameters\n",
    "early_stopping_rounds = 200\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_accuracy = float('inf')\n",
    "\n",
    "shuffle_test_train()\n",
    "\n",
    "for epoch in tqdm(range(20)):\n",
    "    # for batch in tqdm(train):\n",
    "    # print(\"Epoch:\", epoch)\n",
    "    for batch in imb_train_cifar10_ds:\n",
    "        with tf.GradientTape() as tape:\n",
    "            # run network\n",
    "            # x = tf.reshape(tf.cast(batch['image'], tf.float32)/255.0, [-1, 784])\n",
    "            image = tf.cast(tf.expand_dims(batch['image'], axis=0), tf.float32)\n",
    "            labels = batch['label']\n",
    "            logits = conv_classifier(image)\n",
    "            \n",
    "            # print(tf.squeeze(logits, axis=0))\n",
    "            # print(labels)\n",
    "            \n",
    "            # calculate loss\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.squeeze(logits, axis=0), labels=labels)\n",
    "        loss_values.append(loss)\n",
    "        \n",
    "        # gradient update\n",
    "        grads = tape.gradient(loss, conv_classifier.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, conv_classifier.trainable_variables))\n",
    "        \n",
    "        # calculate accuracy\n",
    "        predictions = tf.argmax(logits, axis=1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "        accuracy_values.append(accuracy)\n",
    "        \n",
    "        # Early stopping\n",
    "        if accuracy.numpy() < best_accuracy and np.mean(loss_values) < best_loss:\n",
    "            best_accuracy = accuracy.numpy()\n",
    "            best_loss = np.mean(loss_values)\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        if counter >= early_stopping_rounds:\n",
    "            # print(\"Early stopping!\")\n",
    "            break\n",
    "            \n",
    "# accuracy\n",
    "print(\"Accuracy During Training:\", np.mean(accuracy_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "55540e12-afd2-4afa-9b3a-2c74bf948102",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping!\n",
      "Accuracy During Training: 0.0\n"
     ]
    }
   ],
   "source": [
    "loss_values_t = []\n",
    "accuracy_values_t = []\n",
    "# Early Stopping Parameters\n",
    "early_stopping_rounds = 5\n",
    "best_loss = float('inf')\n",
    "counter = 0\n",
    "best_accuracy = float('inf')\n",
    "\n",
    "for batch in test_cifar10_ds:\n",
    "    with tf.GradientTape() as tape:\n",
    "        # run network\n",
    "        # x = tf.reshape(tf.cast(batch['image'], tf.float32)/255.0, [-1, 784])\n",
    "        image = tf.cast(tf.expand_dims(batch['image'], axis=0), tf.float32)\n",
    "        labels = batch['label']\n",
    "        logits = conv_classifier(image)\n",
    "\n",
    "        # print(tf.squeeze(logits, axis=0))\n",
    "        # print(labels)\n",
    "\n",
    "        # calculate loss\n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.squeeze(logits, axis=0), labels=labels)\n",
    "    loss_values_t.append(loss)\n",
    "\n",
    "    # calculate accuracy\n",
    "    predictions = tf.argmax(logits, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "    accuracy_values_t.append(accuracy)\n",
    "\n",
    "    # Early stopping\n",
    "    if accuracy.numpy() < best_accuracy and np.mean(loss_values_t) < best_loss:\n",
    "        best_accuracy = accuracy.numpy()\n",
    "        best_loss = np.mean(loss_values_t)\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "    if counter >= early_stopping_rounds:\n",
    "        print(\"Early stopping!\")\n",
    "        break\n",
    "            \n",
    "# accuracy\n",
    "print(\"Accuracy During Training:\", np.mean(accuracy_values_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e9665c-8792-40c9-9f65-71838a202aff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CSCE479 (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
