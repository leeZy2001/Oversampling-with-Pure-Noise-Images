{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fb1db349-1bf7-4e7d-a7ce-c0d91a343ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import matplotlib.pyplot as plt    # to visualize data and draw plots\n",
    "from tqdm import tqdm              # to track progress of loops\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d713bf0-c7c2-4a96-8ce7-df1c04cce7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the CIFAR-100 test dataset for testing.\n",
    "def prepare_test_cifar100(): \n",
    "    test_ds = tfds.load('cifar100', split='test')\n",
    "    \n",
    "    prep_test_ds = []\n",
    "    for data in test_ds: \n",
    "        prep_test_ds.append(data)\n",
    "        \n",
    "    return prep_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b9a67e47-44d4-448a-a486-0cddf7a7151a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the balanced CIFAR-100 train dataset for training. \n",
    "# NOTE: Probably did it in the most inefficient way possible. \n",
    "def prepare_train_cifar100(): \n",
    "    # Load CIFAR-10 dataset\n",
    "    train_ds, info = tfds.load('cifar100', split='train', with_info=True)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = info.features['label'].names\n",
    "\n",
    "    # Create lists to hold images and labels in order from class 0 to class 9\n",
    "    ordered_images = [[] for _ in range(100)]\n",
    "    ordered_labels = [[] for _ in range(100)]\n",
    "\n",
    "    # Iterate through the dataset and sort images and labels\n",
    "    for example in train_ds:\n",
    "        image, label = example['image'], example['label']\n",
    "        ordered_images[label].append(image)\n",
    "        ordered_labels[label].append(example['label'])\n",
    "\n",
    "    # Concatenate lists\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(100):\n",
    "        images.extend(ordered_images[i])\n",
    "        labels.extend(ordered_labels[i])\n",
    "\n",
    "    # Convert lists to TensorFlow tensors\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "    \n",
    "    prep_train_ds = []\n",
    "    for i in range(50000): \n",
    "        prep_train_ds.append({\n",
    "            'id': i,\n",
    "            'image': images[i],\n",
    "            'label': labels[i]\n",
    "        })\n",
    "        \n",
    "    return prep_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5c489735-55a4-466d-857e-3da2722fa434",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the balanced CIFAR-10 train dataset as a parameter and imbalances it \n",
    "def prepare_imb_train_cifar100(bal_dataset):\n",
    "    prep_imb_train_ds = []\n",
    "    balanced_dataset = bal_dataset\n",
    "    \n",
    "    #Remove a certain amount of data for each class in CIFAR-100. The amount of data removed is incremated by a rate of 0.01.\n",
    "    for i in range(100): \n",
    "        prep_imb_train_ds.append(balanced_dataset[(500*i):(500 *(i+1))])\n",
    "\n",
    "        number_remove = int(500 * (0.01*i))\n",
    "        del prep_imb_train_ds[i][:number_remove]\n",
    "    \n",
    "    prep_imb_train_ds = sum(prep_imb_train_ds, [])\n",
    "    \n",
    "    return prep_imb_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "812ce953-1172-4afb-b7df-33bdfcd0222e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(lst):\n",
    "    random.shuffle(lst)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6243c19c-5e8b-486a-a888-d6d0ac2ecb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cifar100_ds = prepare_test_cifar10()\n",
    "train_cifar100_ds = prepare_train_cifar100()\n",
    "imb_train_cifar100_ds = prepare_imb_train_cifar100(train_cifar100_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "9c99d989-2111-4493-851d-00a090e4429c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(image_tensor, mean=0.0, stddev=1.0):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to the input image tensor.\n",
    "\n",
    "    Args:\n",
    "    - image_tensor: TensorFlow image tensor of shape (height, width, channels)\n",
    "    - mean: Mean of the Gaussian noise\n",
    "    - stddev: Standard deviation of the Gaussian noise\n",
    "\n",
    "    Returns:\n",
    "    - Noisy image tensor\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal(shape=tf.shape(image_tensor), mean=mean, stddev=stddev, dtype=tf.float32)\n",
    "    noisy_image = tf.cast(image_tensor, dtype=tf.float32) + noise\n",
    "    noisy_image = tf.clip_by_value(noisy_image, 0.0, 255.0)  # Clip values to [0, 255]\n",
    "    noisy_image = tf.cast(noisy_image, dtype=tf.uint8)  # Convert back to uint8\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "a6fe773b-9c01-451a-b910-52a93666876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_open_cifar100(imb_dataset): \n",
    "    increments = [0, 500, 995, 1485, 1970, 2450, 2925, 3395, 3860, 4320, \n",
    "                  4775, 5225, 5670, 6110, 6545, 6975, 7400, 7820, 8235, 8645, \n",
    "                  9050, 9450, 9845, 10235, 10620, 11000, 11375, 11745, 12110, 12470, \n",
    "                  12825, 13175, 13520, 13860, 14195, 14525, 14850, 15170, 15485, 15795, \n",
    "                  16100, 16400, 16695, 16985, 17270, 17550, 17825, 18095, 18360, 18620, \n",
    "                  18875, 19125, 19370, 19610, 19845, 20075, 20300, 20520, 20735, 20945, \n",
    "                  21150, 21350, 21545, 21735, 21920, 22100, 22275, 22445, 22610, 22770, \n",
    "                  22925, 23075, 23220, 23360, 23495, 23625, 23750, 23870, 23985, 24095, \n",
    "                  24200, 24300, 24395, 24485, 24570, 24650, 24725, 24795, 24860, 24920, \n",
    "                  24975, 25025, 25070, 25110, 25145, 25175, 25200, 25220, 25235, 25245, 25250]\n",
    "    classes_dataset = []\n",
    "    \n",
    "    #Separate the dataset into ten datasets based on their class labels\n",
    "    for i in range(len(increments)): \n",
    "        init = increments[i]\n",
    "        \n",
    "        # Get the next increment if it exists\n",
    "        if i < len(increments) - 1:\n",
    "            last = increments[i + 1]\n",
    "        else:\n",
    "            # If we're at the last increment, break the loop\n",
    "            break\n",
    "        \n",
    "        classes_dataset.append(imb_dataset[init:last])\n",
    "    \n",
    "    #Add the oversampled images and pure noise images\n",
    "    for class_dataset in classes_dataset: \n",
    "        num_of_oversampled = 500 - len(class_dataset)\n",
    "        num_of_pure_noise = max(1, int(num_of_oversampled * 0.1))\n",
    "        i = 0\n",
    "        \n",
    "        while(i < num_of_oversampled):\n",
    "            if(i < (num_of_oversampled - num_of_pure_noise)): \n",
    "                class_dataset.append(class_dataset[0])\n",
    "            else: \n",
    "                # Append pure noise image into class_dataset\n",
    "                noisy_image = add_gaussian_noise(class_dataset[0]['image'])\n",
    "                class_dataset.append({\n",
    "                    'id': class_dataset[0]['id'],\n",
    "                    'image': noisy_image,\n",
    "                    'label': class_dataset[0]['label']\n",
    "                })\n",
    "                \n",
    "            i+=1\n",
    "    \n",
    "    open_train_cifar100_ds = sum(classes_dataset, [])\n",
    "    \n",
    "    return open_train_cifar100_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7401b98-79eb-4f64-b5af-781b315b3424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CSCE479 (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
