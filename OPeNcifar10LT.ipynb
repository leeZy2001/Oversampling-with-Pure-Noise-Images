{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f244ef-b4b7-48dc-a794-ddb452a03b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll start with our library imports...\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np                 # to use numpy arrays\n",
    "import tensorflow as tf            # to specify and run computation graphs\n",
    "import tensorflow_datasets as tfds # to load training data\n",
    "import matplotlib.pyplot as plt    # to visualize data and draw plots\n",
    "from tqdm import tqdm              # to track progress of loops\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "506c5b98-45e3-4824-8e64-40df28143193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the CIFAR-10 test dataset for testing.\n",
    "def prepare_test_cifar10(): \n",
    "    test_ds = tfds.load('cifar10', split='test')\n",
    "    \n",
    "    prep_test_ds = []\n",
    "    for data in test_ds: \n",
    "        prep_test_ds.append(data)\n",
    "        \n",
    "    return prep_test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31fcb6e1-4b14-41ae-ab01-422eb3a19312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepares the balanced CIFAR-10 train dataset for training. \n",
    "# NOTE: Probably did it in the most inefficient way possible. \n",
    "def prepare_train_cifar10(): \n",
    "    # Load CIFAR-10 dataset\n",
    "    train_ds, info = tfds.load('cifar10', split='train', with_info=True)\n",
    "\n",
    "    # Get class names\n",
    "    class_names = info.features['label'].names\n",
    "\n",
    "    # Create lists to hold images and labels in order from class 0 to class 9\n",
    "    ordered_images = [[] for _ in range(10)]\n",
    "    ordered_labels = [[] for _ in range(10)]\n",
    "\n",
    "    # Iterate through the dataset and sort images and labels\n",
    "    for example in train_ds:\n",
    "        image, label = example['image'], example['label']\n",
    "        ordered_images[label].append(image)\n",
    "        ordered_labels[label].append(example['label'])\n",
    "\n",
    "    # Concatenate lists\n",
    "    images = []\n",
    "    labels = []\n",
    "    for i in range(10):\n",
    "        images.extend(ordered_images[i])\n",
    "        labels.extend(ordered_labels[i])\n",
    "\n",
    "    # Convert lists to TensorFlow tensors\n",
    "    images = tf.convert_to_tensor(images)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "    \n",
    "    prep_train_ds = []\n",
    "    for i in range(50000): \n",
    "        prep_train_ds.append({\n",
    "            'id': i,\n",
    "            'image': images[i],\n",
    "            'label': labels[i]\n",
    "        })\n",
    "        \n",
    "    return prep_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca6a4f59-9f6c-4eac-b88a-2e6a3438561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Takes the balanced CIFAR-10 train dataset as a parameter and imbalances it \n",
    "def prepare_imb_train_cifar10(bal_dataset):\n",
    "    prep_imb_train_ds = []\n",
    "    balanced_dataset = bal_dataset\n",
    "    \n",
    "    #Remove a certain amount of data for each class in CIFAR-10. The amount of data removed is incremated by a rate of 0.1.\n",
    "    for i in range(10): \n",
    "        prep_imb_train_ds.append(balanced_dataset[(5000*i):(5000 *(i+1))])\n",
    "\n",
    "        number_remove = int(5000 * (0.1*i))\n",
    "        del prep_imb_train_ds[i][:number_remove]\n",
    "    \n",
    "    prep_imb_train_ds = sum(prep_imb_train_ds, [])\n",
    "    \n",
    "    return prep_imb_train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5440b845-8e20-4be4-8fe0-882fa82bc052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(lst):\n",
    "    random.shuffle(lst)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6da5cf0d-1ec8-4ccf-813b-1f72dc759bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cifar10_ds = prepare_test_cifar10()\n",
    "train_cifar10_ds = prepare_train_cifar10()\n",
    "imb_train_cifar10_ds = prepare_imb_train_cifar10(train_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "175cf6ad-739e-4114-9f00-ee93bf14e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_gaussian_noise(image_tensor, mean=0.0, stddev=1.0):\n",
    "    \"\"\"\n",
    "    Add Gaussian noise to the input image tensor.\n",
    "\n",
    "    Args:\n",
    "    - image_tensor: TensorFlow image tensor of shape (height, width, channels)\n",
    "    - mean: Mean of the Gaussian noise\n",
    "    - stddev: Standard deviation of the Gaussian noise\n",
    "\n",
    "    Returns:\n",
    "    - Noisy image tensor\n",
    "    \"\"\"\n",
    "    noise = tf.random.normal(shape=tf.shape(image_tensor), mean=mean, stddev=stddev, dtype=tf.float32)\n",
    "    noisy_image = tf.cast(image_tensor, dtype=tf.float32) + noise\n",
    "    noisy_image = tf.clip_by_value(noisy_image, 0.0, 255.0)  # Clip values to [0, 255]\n",
    "    noisy_image = tf.cast(noisy_image, dtype=tf.uint8)  # Convert back to uint8\n",
    "    return noisy_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d3f582eb-5221-4513-83aa-22569dd0cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_open_cifar10(imb_dataset): \n",
    "    increments = [0, 5000, 9500, 13500, 17000, 20000, 22500, 24500, 26000, 27000, 27500]\n",
    "    classes_dataset = []\n",
    "    \n",
    "    #Separate the dataset into ten datasets based on their class labels\n",
    "    for i in range(len(increments)): \n",
    "        init = increments[i]\n",
    "        \n",
    "        # Get the next increment if it exists\n",
    "        if i < len(increments) - 1:\n",
    "            last = increments[i + 1]\n",
    "        else:\n",
    "            # If we're at the last increment, break the loop\n",
    "            break\n",
    "        \n",
    "        classes_dataset.append(imb_dataset[init:last])\n",
    "    \n",
    "    #Add the oversampled images and pure noise images\n",
    "    for class_dataset in classes_dataset: \n",
    "        num_of_oversampled = 5000 - len(class_dataset)\n",
    "        num_of_pure_noise = int(num_of_oversampled * 0.1)\n",
    "        i = 0\n",
    "        \n",
    "        while(i < num_of_oversampled):\n",
    "            if(i < (num_of_oversampled - num_of_pure_noise)): \n",
    "                class_dataset.append(class_dataset[0])\n",
    "            else: \n",
    "                # Append pure noise image into class_dataset\n",
    "                noisy_image = add_gaussian_noise(class_dataset[0]['image'])\n",
    "                class_dataset.append({\n",
    "                    'id': class_dataset[0]['id'],\n",
    "                    'image': noisy_image,\n",
    "                    'label': class_dataset[0]['label']\n",
    "                })\n",
    "                \n",
    "            i+=1\n",
    "    \n",
    "    open_train_cifar10_ds = sum(classes_dataset, [])\n",
    "    \n",
    "    return open_train_cifar10_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "046659ad-0adc-4692-8da5-acb13cbee662",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### MODEL ARCH #########\n",
    "hidden_1 = tf.keras.layers.Conv2D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "hidden_2 = tf.keras.layers.Conv2D(filters=64, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "pool_1 = tf.keras.layers.MaxPool2D(padding='same')\n",
    "hidden_3 = tf.keras.layers.Conv2D(filters=128, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "hidden_4 = tf.keras.layers.Conv2D(filters=256, kernel_size=3, padding='same', activation=tf.nn.relu)\n",
    "pool_2 = tf.keras.layers.MaxPool2D(padding='same')\n",
    "flatten = tf.keras.layers.Flatten()\n",
    "output = tf.keras.layers.Dense(10)\n",
    "conv_classifier = tf.keras.Sequential([hidden_1, hidden_2, pool_1, hidden_3, hidden_4, pool_2, flatten, output])\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46250e5a-eded-4437-a3c6-80db51270e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_train_cifar10_ds = prepare_open_cifar10(imb_train_cifar10_ds)\n",
    "\n",
    "def shuffle_test_train(): \n",
    "    shuffle_list(test_cifar10_ds)\n",
    "    shuffle_list(train_cifar10_ds)\n",
    "    shuffle_list(imb_train_cifar10_ds)\n",
    "    shuffle_list(open_train_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f19657b-9dbf-448a-a80d-ee3473ae3103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_train(dataset):\n",
    "    loss_values = []\n",
    "    accuracy_values = []\n",
    "    # Early Stopping Parameters\n",
    "    early_stopping_rounds = 200\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_accuracy = float('inf')\n",
    "\n",
    "    # shuffle_test_train()\n",
    "    shuffle_list(dataset)\n",
    "\n",
    "    for batch in tqdm(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # run network\n",
    "            image = tf.cast(tf.expand_dims(batch['image'], axis=0), tf.float32)\n",
    "            labels = batch['label']\n",
    "            logits = conv_classifier(image)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.squeeze(logits, axis=0), labels=labels)\n",
    "        loss_values.append(loss)\n",
    "\n",
    "        # gradient update\n",
    "        grads = tape.gradient(loss, conv_classifier.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, conv_classifier.trainable_variables))\n",
    "\n",
    "        # calculate accuracy\n",
    "        predictions = tf.argmax(logits, axis=1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "        accuracy_values.append(accuracy)\n",
    "\n",
    "    # accuracy\n",
    "    print(\"Accuracy During Training:\", np.mean(accuracy_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "45ca83f3-653d-402c-be35-01a97f5f8d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model_test(dataset): \n",
    "    loss_values_t = []\n",
    "    accuracy_values_t = []\n",
    "    # Early Stopping Parameters\n",
    "    early_stopping_rounds = 5\n",
    "    best_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_accuracy = float('inf')\n",
    "    \n",
    "    shuffle_list(dataset)\n",
    "\n",
    "    for batch in tqdm(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # run network\n",
    "            image = tf.cast(tf.expand_dims(batch['image'], axis=0), tf.float32)\n",
    "            labels = batch['label']\n",
    "            logits = conv_classifier(image)\n",
    "\n",
    "            # print(tf.squeeze(logits, axis=0))\n",
    "            # print(labels)\n",
    "\n",
    "            # calculate loss\n",
    "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tf.squeeze(logits, axis=0), labels=labels)\n",
    "        loss_values_t.append(loss)\n",
    "\n",
    "        # calculate accuracy\n",
    "        predictions = tf.argmax(logits, axis=1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "        accuracy_values_t.append(accuracy)\n",
    "\n",
    "    # accuracy\n",
    "    # print(accuracy_values_t)\n",
    "    print(\"Accuracy During Training:\", np.mean(accuracy_values_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b24e24e-13bd-45f3-87d6-48c29e796d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith train_cifar10_ds \\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "With train_cifar10_ds \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c026f0b-dc03-4e7f-868e-39adcc1252b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [04:12<00:00, 198.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy During Training: 0.28924\n"
     ]
    }
   ],
   "source": [
    "conv_model_train(train_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5df050bf-3aec-49ee-bfba-ab3359bc395a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 462.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy During Training: 0.3365\n"
     ]
    }
   ],
   "source": [
    "conv_model_test(test_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b3be0d4-db3d-48ec-9265-bbf9ce30d545",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith imb_train_cifar10_ds \\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "With imb_train_cifar10_ds \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7afb9530-27eb-47b6-bab6-06252108cc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 27500/27500 [02:18<00:00, 199.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy During Training: 0.37832728\n"
     ]
    }
   ],
   "source": [
    "conv_model_train(imb_train_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "41108850-148a-44de-b819-5cfb67f500a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 470.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy During Training: 0.2775\n"
     ]
    }
   ],
   "source": [
    "conv_model_test(test_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1fd4f21-8357-438f-8c82-88c2ffd1e048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWith open_train_cifar10_ds \\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "With open_train_cifar10_ds \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0706e56-3791-46dd-b1ec-cb3304bf0b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [04:06<00:00, 203.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy During Training: 0.64746\n"
     ]
    }
   ],
   "source": [
    "conv_model_train(open_train_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "85931719-9485-4152-959d-0aeaf20e8bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:21<00:00, 474.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy During Training: 0.2612\n"
     ]
    }
   ],
   "source": [
    "conv_model_test(test_cifar10_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f78f871-4ad0-4c0d-8d8e-3e080d687610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python CSCE479 (tensorflow-env)",
   "language": "python",
   "name": "tensorflow-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
